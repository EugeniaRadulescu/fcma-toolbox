		<strong>Analysis stage:</strong> The first stage, <em>voxel selection</em>, does most of the heavy lifting. It computes correlation matrices for every block and runs classification on every voxel in mask1, so is the most compute-intensive stage. The result is a sorted list of voxels, in decreasing order of predictive power assessed by an SVM classifier during training (the sequence and score are also saved as NIfTI volumes). A third stage, <em>visualization</em> simply saves all the voxel-voxel correlations for a particular block (See <em>Correlation Visualization</em> section below for details). The second stage, <em>testing predictive accuracy</em> tests the voxels from stage 1 using the output of stage 1, in different ways depending on the following 2 settings.<br>
		<strong>Test increasing top voxels</strong>. The list of top voxels from stage1 (the file ending in <code>_list.txt</code>) can be used to assess how classification changes with an increasing number of top voxels from the list. Top 10, 50, 100, 200, 500, 1000 ... 5000 voxels are tested and results reported for each of them.<br>
		<strong>Test via cross validation</strong>. If selected, a series of tests will be performed, each with a different set of blocks held out (set size same as number of holds set below) and the total percent correct reported. If not set, one test is performed for each block held: the first  . The result will be either correct or incorrect (presumably you'd have multiple sets of blockfiles and/or datafiles to test in order to compute total percent correct from multiple iterations of the program.) Whether set or not, the NIfTI volumes output from stage 1 can be used as mask1 with a wholebrain mask as mask2. The output files end in <code>_seq.nii.gz</code> and <code>_scores.nii.gz</code>, and can be thresholded first based on the top voxel test described above.<br>
		<strong>Number of folds:</strong> Only applies to stage 1, voxel selection. Voxels are rated based on how they fare in a k-fold SVM cross-validation which divides data into k subsets. A good choice for k is the number of subjects if comparing across subjects, or runs if comparing across runs, etc. This way training data will be roughly balanced across sources. The traditional defaults are 5 and 10 but the number of blocks (trials x subjects) will be much higher than that in our domain.<br>
		<strong>Holds for selection or cross-validation:</strong> During selection (stage 1) you can leave out a particular string of trials (beginning with a first block with index in the range <code>[0,subjects x blocks/subject]</code>) from the selection process. Alternately you can remove those blocks from the data (and corresponding blocks files). When testing prediction accuracy (stage 2) a test is performed on as many blocks as are held from the end of the block list. Predictions are based on the first <code>trials - holds</code> blocks, and tested on the remaining <code>holds</code> blocks. If cross validating, every permutation of <code>trials - holds vs holds</code> is used for prediction + testing. Trial (ie block id) numbering is across subjects: if there are 12 blocks per subject, block ID "0" is subject1_block1, "1" is  subject1_block2 ... "11" is subject1_block12, "12" is subject2_block1, "13" is subject2_block2 ... <br>
		<strong>Blocks directory/file:</strong> If the same blocks apply to all data files, a single file can be used. <em>If using a blockfile directory, there must be 1:1 correspondence between blocks:data files</strong></em>. The filenames must differ only by file extension (Nifti compressed <code>.nii.gz</code> vs plaintext <code>.txt</code>)<br>
		<em><strong>Blockfile format</strong></em><br>A blockfile starts with the number of blocks on the first line, followed by as many lines. On each line is the label (0 or 1), the start TR, and the ending TR number in the run.
		<br>
		<pre>
N
Label StartTR EndTR
... N rows total ...
Label StartTR EndTR</pre>
		<strong>Data directory:</strong> All files in the data directory are loaded. The number of subjects is set equal to the number of files found. Currently only Nifti compressed (<code>.nii.gz</code> extension) is recognized.<br>
		<strong>Voxel masks:</strong> Zero, one, or two masks can be specified; depending on the task of analysis, a different number may be used. During selection voxels are correlated between mask1 and mask2 regions. If one, then voxels within a region are auto-correlated. No masks will result in all voxels being auto-correlated, not recommended. A wholebrain mask should be provided instead. The same mask can be used as mask1 and mask2. During testing the number of top voxels, only the top voxels from the text list are used, and correlated with voxels in the first mask. Otherwise, the first mask is always used as the list of voxels tested and ranked, vs all the voxels in mask2. Voxel masks must be in Nifti compressed (<code>.nii.gz</code>) format.<br>
		<strong>Output file:</strong> Specify a prefix, which will be used for 3 output files during voxel selection: 1) a <em>[outputfile]_list.txt</em> file with the top-scoring voxels, 2) a <em>[outputfile]_seq.nii.gz</em> file with the voxel rank at its corresponding x,y,z position, and 3) <em>[outputfile]_score.nii.gz</em> which has voxel score at its corresponding x,y,z position. These files can be used directly (or thresholded) as input to the testing stage (the entire filename, not just its prefix, is required when the file is used as input.) The output of prediction accuracy in the second stage is currently written to the console as standard out.<br>
		<strong>Correlation Visualization</strong>. Supply the blockID for which to save all voxel-voxel correlations, and a NIfTI file whose header info can be used for the output file that will contain the saved correlations. Each volume in the 4D output will hold the correlations between one voxel in Mask1 vs all the voxels in Mask2 (if provided). The actual visualization of the correlations is an exercise left to the user, but the goal is to turn this output into <a href="http://circos.ca">circos</a> charts.<br>
		<strong>Classifier:</strong> While <em>SVM prediction</em> should be used, some experimental alternatives are available. Both <em>smart distance ratio</em> and <em>correlation sum</em> are available. Note that if the <em>MVPA control condition</em> is checked (up next), this setting is ignored and a <em>searchlight + svm</em> selection + classifier will be used.<br><br>
		<strong><em>Note: SVM is the default classifier; only binary classification (0/1) is currently supported in the toolbox as a result</em></strong><br>Multiple classes are not hard to add, however, using the standard methods for extending SVM to handle multiple classes (one-to-all, one-to-one). Currently this needs to be be done manually via a set of appropriately constructed blockfiles.<br><br>
		<strong>MVPA control condition:</strong> Assessing the advantage of using correlation patterns vs activation patterns can be explored by running a simple variant of MVPA hardcoded to use searchlight feature selection and SVM performance assessment. The MVPA code may have bugs since it has not been as well-exercised as the rest of the codebase, so please do not rely on this program for MVPA results alone.<br>
