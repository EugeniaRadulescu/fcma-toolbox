		<strong>Analysis stage:</strong> The first stage, <em>voxel selection</em>, does most of the heavy lifting. It computes correlation matrices for every block and runs classification on every voxel in mask1, so is the most compute-intensive stage. The result is a sorted list of voxels, in decreasing order of predictive power assessed by an SVM classifier during training (the sequence and score are also saved as NIfTI volumes). A third stage, <em>visualization</em> simply saves all the voxel-voxel correlations for a particular block (See <em>Correlation Visualization</em> section below for details). The second stage, <em>testing predictive accuracy</em> tests the voxels from stage 1 using the output of stage 1, in different ways depending on the following 2 settings.<br>
		<strong>Test increasing top voxels</strong>. The list of top voxels from stage1 (the file ending in <code>_list.txt</code>) can be used to assess how classification changes with an increasing number of top voxels from the list. Top 10, 50, 100, 200, 500, 1000 ... 5000 voxels are tested and results reported for each of them.<br>
		<strong>Test via cross validation</strong>. If selected, a series of tests will be performed, each with a different set of blocks held out (set size same as number of blocks held out) and the total percent correct reported. If not set, one test is performed for each block held out (see the holds section below). Whether set or not, the NIfTI volumes output from stage 1 can be used as mask1 with a wholebrain mask typically used as mask2. The output files end in <code>_seq.nii.gz</code> and <code>_scores.nii.gz</code>, and should be thresholded first based on the results from increasing the number of top voxels, to prevent overfitting.<br>
		<strong>Number of folds:</strong> This setting is only used in stage 1 (voxel selection). Voxels are rated based on how they fare in a k-fold SVM cross-validation which divides data into k subsets. A good choice for k is the number of subjects (minus one) if comparing across subjects, or runs (minus test runs) if comparing across runs, etc. If holding back blocks, the number of folds should be reduced so that they evenly divide the training (ie non-held) blocks. This way training data will be roughly balanced across sources assuming an equal number of "on" blocks as "off" blocks in the regressors. If so, the number of folds should always be an even number.<br>
		<strong>Holds for selection or cross-validation:</strong> During selection (stage 1) you can leave out a particular string of trials beginning with a first block with index in the range <code>[0,subjects x blocks in blockfile]</code>). Alternately you can remove those blocks from the blockfile(s) altogether (and if you are extra careful not to throw off block numbering, from the data itself). When testing prediction accuracy (stage 2) a test is performed on as many blocks as are held from the end of the block list. Predictions are based on the first <code>trials - holds</code> blocks, and tested on the remaining <code>holds</code> blocks. If cross validating, every permutation of <code>trials - holds vs holds</code> is used for prediction + testing. Trial (ie block id) numbering is across subjects: if there are 12 blocks per subject, block ID "0" is subject1_block1, "1" is  subject1_block2 ... "11" is subject1_block12, "12" is subject2_block1, "13" is subject2_block2 ... <br>
		<strong>Blocks directory/file:</strong> If the same blocks apply to all data files, a single file can be used. <em>If using a blockfile directory, there must be 1:1 correspondence between blockfiles:datafiles</strong></em>. The filenames must be the same except for their file extensions (NIfTI compressed <code>.nii.gz</code> vs plaintext <code>.txt</code>)<br>
		<em><strong>Blockfile format</strong></em><br>A blockfile starts with the number of blocks on the first line, followed by as many lines. On each line is the label (0 or 1), the start TR, and the ending TR number in the run. Any hemodynamic shift would need to be added manually here, but the TR range must consist of two integers, so any shift needs to be whole TRs.
		<br>
		<pre>
N
Label StartTR EndTR
... N rows total ...
Label StartTR EndTR</pre>
		<strong>Data directory:</strong> All files in the data directory are loaded. The number of subjects is set equal to the number of files found. Currently only Nifti compressed (<code>.nii.gz</code> extension) is recognized. Note that excessive data copying can be avoided by using symlinks in this directory, pointing to wherever data files are kept if spread across multiple directories.<br>
		<strong>Voxel masks:</strong> Depending on the task of analysis, a different number and type of mask is used. During selection, voxels are correlated between mask1 and mask2 regions. If both mask1 and mask2 are the same, then voxels within that region are auto-correlated. If no masks are provided, all voxels will be auto-correlated, or the single mask will be correlated with all voxels. Proceeding without a mask is not recommended due to the large number of voxels outside the brain in the typical volume (they will each be correlated and classified!). A wholebrain mask should be provided at the very least. See the various test types above for information about which masks are used in each test case. Voxel masks must be in Nifti compressed (<code>.nii.gz</code>) format.<br>
		<strong>Output file:</strong> Specify a prefix, which will be used for 3 output files during voxel selection: 1) a <em>[outputfile]_list.txt</em> file with the top-scoring voxels, 2) a <em>[outputfile]_seq.nii.gz</em> file with the voxel rank at its corresponding x,y,z position, and 3) <em>[outputfile]_score.nii.gz</em> which has voxel score at its corresponding x,y,z position. These files can be used directly (or thresholded) as input to the testing stage (the entire filename, not just its prefix, is required when the file is used as input.) The output of prediction accuracy in the second stage is currently written to the console as standard out.<br>
		<strong>Correlation Visualization</strong>. Supply the blockID for which to save all voxel-voxel correlations, and a NIfTI file whose header info can be used for the output file that will contain the saved correlations. Each volume in the 4D output will hold the correlations between one voxel in Mask1 vs all the voxels in Mask2 (if provided). The actual visualization of the correlations is an exercise left to the user, but the goal is to turn this output into <a href="http://circos.ca">circos</a> charts.<br>
		<strong>Classifier:</strong> While <em>SVM prediction</em> should be used, some experimental alternatives are available. Both <em>smart distance ratio</em> and <em>correlation sum</em> are available. Note that if the <em>MVPA control condition</em> is checked (up next), this setting is ignored and a <em>searchlight + svm</em> selection + classifier will be used.<br><br>
		<strong><em>Note: SVM is the default classifier; only binary classification (0/1) is currently supported in the toolbox as a result</em></strong><br>Multiple classes are not hard to add, however, using the standard methods for extending SVM to handle multiple classes (one-to-all, one-to-one). Currently this needs to be be done manually via a set of appropriately constructed blockfiles.<br><br>
		<strong>MVPA control condition:</strong> Assessing the advantage of using correlation patterns vs activation patterns can be explored by running a simple variant of MVPA hardcoded to use searchlight feature selection and SVM performance assessment. The MVPA code may have bugs since it has not been as well-exercised as the rest of the codebase, so please do not rely on this program for MVPA results alone.<br>
