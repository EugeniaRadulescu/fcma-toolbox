		<strong>Analysis stage:</strong> The first stage, <em>voxel selection</em>, does most of the heavy lifting. It computes correlation matrices for every block so is the most compute-intensive stage. The result is a sorted list of voxels, in decreasing order of predictive power assessed by an SVM classifier during training. The second stage, <em>testing predictive accuracy</em> tests the voxels from stage 1 via maskfile(s) which must be constructed from stage 1 results. A third stage (optional; can be run at any point) simply saves all the voxel-voxel correlations for a particular block (See <em>Correlation Visualization</em> section below for details)<br>
		<strong>Number of folds:</strong> Only applies to stage 1, voxel selection. Voxels are rated based on how they fare in a k-fold SVM cross-validation which divides data into k subsets. A good choice for k is half the number of blocks in a run, or perhaps the number of subjects. This way training data will be roughly balanced across sources. The traditional defaults are 5 and 10. Values much higher than 10 are discouraged and will increase compute time (model testing is done for each fold.)<br>
		<strong>Blocks held back for cross-validation:</strong> When testing prediction accuracy (stage 2) one can leave out a contiguous set of blocks from the test procedure by specifying the start block ID together with the number of blocks in the contiguous set. The block ID ranges from 0 to subjects x blocks, with all blocks for a given subject appearing in the list before the next subject ("subject" being one of the datafiles in the data directory.) For example, if there are 12 blocks per subject, block ID "0" is subject1_block1, "1" is  subject1_block2 ... "11" is subject1_block12, "12" is subject2_block1, "13" is subject2_block2 ... <br>
		<strong>Blocks directory/file:</strong> If the same blocks apply to all data files, a single file can be used. <strong><em>If using a blockfile directory, there must be 1:1 correspondence between blocks:data files</strong></em>. The filenames must differ only by file extension (Nifti compressed <code>.nii.gz</code> vs plaintext <code>.txt</code>)<br>
		<br>
		<em><strong>Blockfile format</strong></em><br>A blockfile starts with the number of blocks on the first line, followed by as many lines. On each line is the label (0 or 1), the start TR, and the ending TR number in the run.
		<br>
		<pre>
N
Label StartTR EndTR
... N rows total ...
Label StartTR EndTR</pre>
		<strong>Data directory:</strong> All files in the data directory are loaded. The number of subjects is set equal to the number of files found. Currently only Nifti compressed (<code>.nii.gz</code> extension) is recognized.<br>
		<strong>Voxel masks:</strong> Zero, one, or two masks can be specified; if two, then voxels are correlated between regions. If one, then voxels within a region are auto-correlated. No masks will result in all voxels being auto-correlated, not recommended due to the high demand it places on computing resources. Voxel masks must be in Nifti compressed (<code>.nii.gz</code>) format as well.<br>
		<strong>Output file:</strong> Specify a prefix, which will be used for 3 output files during voxel selection: 1) a <em>[outputfile]_list.txt</em> file with the top-scoring voxels, 2) a <em>[outputfile]_seq.nii.gz</em> file with the voxel rank at its corresponding x,y,z position, and 3) <em>[outputfile]_score.nii.gz</em> which has voxel score at its corresponding x,y,z position. These files should be used as the basis for constructing the Nifti-formatted masks to be used as input to the testing stage (can be used as-is but thresholded to remove voxels below a score or rank threshold.) The output of prediction accuracy in the second stage is currently written to standard out and must be parsed from the job log files. This inconvenience will be fixed in the next update to the toolbox.<br>
		<strong>Correlation Visualization</strong>. The blockID for which to save all voxel-voxel correlations, and the prefix name of the 4D file to save the correlations. Each volume in the 4D file will hold the correlations between one voxel in Mask1 with all the voxels in Mask2 (if provided). The actual visualization of the correlations is an exercise left to the user (for now.)
		<strong>Classifier:</strong> While <em>SVM prediction</em> should be used, some experimental alternatives are available. Both <em>smart distance ratio</em> and <em>correlation sum</em> are quick to compute though accuracy will be lower. Note that if the <em>MVPA control condition</em> is checked (up next), this setting is ignored and a <em>searchlight + svm</em> selection + classifier will be used.<br><br>
		<strong><em>Note: SVM is the default classifier; only binary classification (0/1) is currently supported in the toolbox as a result</em></strong><br>Multiple classes are not hard to add, however, using the standard methods for extending SVM to handle multiple classes (one-to-all, one-to-one). However the computation time would likely grow substantially.<br><br>
		<strong>MVPA control condition:</strong> Assessing the advantage of using correlation patterns vs activation patterns can be explored by running a simple variant of MVPA hardcoded to use searchlight feature selection and SVM performance assessment.<br>
